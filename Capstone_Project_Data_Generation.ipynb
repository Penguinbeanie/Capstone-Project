{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32dde54f17f1456eb4d2fc20668c7c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b4732b0e4fe4153af55acb8d76bde92",
              "IPY_MODEL_6c500b72b1d645d4a2fc7b034adff57f",
              "IPY_MODEL_8d293aa7f918422596e95e5026f7404d"
            ],
            "layout": "IPY_MODEL_aa59f9aefba04965a1fe5bd9ab8e5274"
          }
        },
        "1b4732b0e4fe4153af55acb8d76bde92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a917f9082b8749549383fd68bd81a6f1",
            "placeholder": "​",
            "style": "IPY_MODEL_bf2bc25df11349ae8cdd704b02dab5a6",
            "value": "config.json: 100%"
          }
        },
        "6c500b72b1d645d4a2fc7b034adff57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bace8ecf2c84054a86dbce93136712f",
            "max": 1154,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cceb7827f7454d61affd260489e85954",
            "value": 1154
          }
        },
        "8d293aa7f918422596e95e5026f7404d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfd3712ce42b4bf99604ff0c453277c5",
            "placeholder": "​",
            "style": "IPY_MODEL_3b771b508cee4bc58530183e3986cb3b",
            "value": " 1.15k/1.15k [00:00&lt;00:00, 68.8kB/s]"
          }
        },
        "aa59f9aefba04965a1fe5bd9ab8e5274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a917f9082b8749549383fd68bd81a6f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf2bc25df11349ae8cdd704b02dab5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bace8ecf2c84054a86dbce93136712f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cceb7827f7454d61affd260489e85954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfd3712ce42b4bf99604ff0c453277c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b771b508cee4bc58530183e3986cb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2be75242495e42618a770fec255199c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34b99293ee3647048f8c19d9e12a822a",
              "IPY_MODEL_61f89768909f482ab193aa768eaf8e8d",
              "IPY_MODEL_1c3fc40e67b3441d8ca2005864d3711f"
            ],
            "layout": "IPY_MODEL_c3f83efcb20145439810844e3fe0f3de"
          }
        },
        "34b99293ee3647048f8c19d9e12a822a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc0e8fb98d3d4427bded5e29b3558029",
            "placeholder": "​",
            "style": "IPY_MODEL_85875d0eb2524e16ad7a81fdb519f65d",
            "value": "model.safetensors: 100%"
          }
        },
        "61f89768909f482ab193aa768eaf8e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19051863e0a54d62b0cfd0e3cd94e934",
            "max": 1629437147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_510d26b2e0d64832ba01044e93bfe3d5",
            "value": 1629437147
          }
        },
        "1c3fc40e67b3441d8ca2005864d3711f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_788c3515c0aa4dc5b4c3fcd805e36a9d",
            "placeholder": "​",
            "style": "IPY_MODEL_e3e02c0093364e70a55062b6ee8ef8a0",
            "value": " 1.63G/1.63G [00:10&lt;00:00, 59.3MB/s]"
          }
        },
        "c3f83efcb20145439810844e3fe0f3de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc0e8fb98d3d4427bded5e29b3558029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85875d0eb2524e16ad7a81fdb519f65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19051863e0a54d62b0cfd0e3cd94e934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "510d26b2e0d64832ba01044e93bfe3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "788c3515c0aa4dc5b4c3fcd805e36a9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3e02c0093364e70a55062b6ee8ef8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b20644889614d9597883b6c7ed7c4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_066e674fdac44fa29c6895fc955c3e3d",
              "IPY_MODEL_23ab41fe61fe4442ae33de4537e038f6",
              "IPY_MODEL_ba7d60018ec74fe39c5a4b7437ae4217"
            ],
            "layout": "IPY_MODEL_86eab91cc6124b4e9dacdd52897e2fa6"
          }
        },
        "066e674fdac44fa29c6895fc955c3e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa8fdde1a614c6090944b0e770d7db4",
            "placeholder": "​",
            "style": "IPY_MODEL_e752cddbb17c46c880d097218d270288",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "23ab41fe61fe4442ae33de4537e038f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce36a408a4054a17a870bdc4b273536a",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42161ed80d13494f91a66e6667b0a525",
            "value": 26
          }
        },
        "ba7d60018ec74fe39c5a4b7437ae4217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a002661a939643f2bc3c36ba43ff8627",
            "placeholder": "​",
            "style": "IPY_MODEL_428f9c7290e2400890771d9993fdd3db",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.97kB/s]"
          }
        },
        "86eab91cc6124b4e9dacdd52897e2fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fa8fdde1a614c6090944b0e770d7db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e752cddbb17c46c880d097218d270288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce36a408a4054a17a870bdc4b273536a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42161ed80d13494f91a66e6667b0a525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a002661a939643f2bc3c36ba43ff8627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "428f9c7290e2400890771d9993fdd3db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "528af380945445aba679e635bfd113c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebf6bbb4ecf5436b9a564c6e436c8e09",
              "IPY_MODEL_82d81624f5b44b7eac46e99ea1bebc32",
              "IPY_MODEL_5610a4cbb2564634bc05c61476c7df2c"
            ],
            "layout": "IPY_MODEL_98be7b9e58074c8fb2d97213bea9fec0"
          }
        },
        "ebf6bbb4ecf5436b9a564c6e436c8e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000221914f414d01895e767eb84c411e",
            "placeholder": "​",
            "style": "IPY_MODEL_6cff377157e146558fb2296733c47865",
            "value": "vocab.json: 100%"
          }
        },
        "82d81624f5b44b7eac46e99ea1bebc32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9772fca83ded4aedbe296f051253dd19",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5fa67e0a84944a08d35b3acf8d86b9b",
            "value": 898822
          }
        },
        "5610a4cbb2564634bc05c61476c7df2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d06115dad61a47d4a41c7ae38a8bfc4a",
            "placeholder": "​",
            "style": "IPY_MODEL_1c97a00d8e7442349f313354611f902e",
            "value": " 899k/899k [00:00&lt;00:00, 6.55MB/s]"
          }
        },
        "98be7b9e58074c8fb2d97213bea9fec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "000221914f414d01895e767eb84c411e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cff377157e146558fb2296733c47865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9772fca83ded4aedbe296f051253dd19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fa67e0a84944a08d35b3acf8d86b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d06115dad61a47d4a41c7ae38a8bfc4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c97a00d8e7442349f313354611f902e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d947e3eac0164714a74011f35d0af3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36ff5c0fa7d84845b5fef1873c825d3a",
              "IPY_MODEL_a7b1cf12477244e3b07d0f01c64e5bb6",
              "IPY_MODEL_198633380b344515af498b648ccbc189"
            ],
            "layout": "IPY_MODEL_7812dd0f38a74993af46b9764cf27e5e"
          }
        },
        "36ff5c0fa7d84845b5fef1873c825d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef66afd03734aba8de28d8e577a3413",
            "placeholder": "​",
            "style": "IPY_MODEL_8ac6506e615144739d4a3094fc339ac5",
            "value": "merges.txt: 100%"
          }
        },
        "a7b1cf12477244e3b07d0f01c64e5bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceedbd90ed884e47b9957ad39b6759ec",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b22906a7e6cb4b80850218e0ba37aab1",
            "value": 456318
          }
        },
        "198633380b344515af498b648ccbc189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d92e6a03894ac286fac2543f938822",
            "placeholder": "​",
            "style": "IPY_MODEL_57a30954650949c4b5eb3572c1032c94",
            "value": " 456k/456k [00:00&lt;00:00, 6.68MB/s]"
          }
        },
        "7812dd0f38a74993af46b9764cf27e5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef66afd03734aba8de28d8e577a3413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ac6506e615144739d4a3094fc339ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceedbd90ed884e47b9957ad39b6759ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b22906a7e6cb4b80850218e0ba37aab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33d92e6a03894ac286fac2543f938822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a30954650949c4b5eb3572c1032c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be5527776682455eb8f90a5b688df22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd7f7dc02d644650bb00100f0f63aa07",
              "IPY_MODEL_54a6e9c1779f480f9bb1594838291607",
              "IPY_MODEL_72c7aa9180434d078696416893683137"
            ],
            "layout": "IPY_MODEL_b199a1d8681e4df9a7b83841890e9a7d"
          }
        },
        "fd7f7dc02d644650bb00100f0f63aa07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc2f4a4edcd43139aee28b4a962e20f",
            "placeholder": "​",
            "style": "IPY_MODEL_26e030d7be0343678d18564ca4e61abe",
            "value": "tokenizer.json: 100%"
          }
        },
        "54a6e9c1779f480f9bb1594838291607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_307c1b938ff74d59bd7396d5a75591b7",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bfb16c4ca414e01a608b365ed34ef18",
            "value": 1355863
          }
        },
        "72c7aa9180434d078696416893683137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473afd9fee9942c99b777456780fee83",
            "placeholder": "​",
            "style": "IPY_MODEL_222e0077e7b54ca59e82f53eab701a3f",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.70MB/s]"
          }
        },
        "b199a1d8681e4df9a7b83841890e9a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc2f4a4edcd43139aee28b4a962e20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e030d7be0343678d18564ca4e61abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "307c1b938ff74d59bd7396d5a75591b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bfb16c4ca414e01a608b365ed34ef18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "473afd9fee9942c99b777456780fee83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "222e0077e7b54ca59e82f53eab701a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Penguinbeanie/Capstone-Project/blob/dev_branch/Capstone_Project_Data_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Model"
      ],
      "metadata": {
        "id": "cv0Pr_-GUxdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP43IcWEagqd"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import userdata\n",
        "import json\n",
        "\n",
        "# retrieving the key stored in Colab\n",
        "key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# configure the key for calling GenAI model\n",
        "genai.configure(api_key=key)\n",
        "\n",
        "# load model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sorting Original Questionnaire Questions by Question Type"
      ],
      "metadata": {
        "id": "LdNtU4F5U2QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from google.colab import files\n",
        "\n",
        "input_files = [\n",
        "    \"https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/original_files/questionnaire1.json\",\n",
        "    \"https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/original_files/questionnaire2.json\",\n",
        "    \"https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/original_files/questionnaire3.json\",\n",
        "    \"https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/original_files/questionnaire4.json\",\n",
        "    \"https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/original_files/questionnaire5.json\"\n",
        "]\n",
        "\n",
        "# Dictionary to hold questions grouped by type\n",
        "question_types = {}\n",
        "\n",
        "# Iterate through each file\n",
        "for file_url in input_files:\n",
        "    response = requests.get(file_url)\n",
        "    data = response.json()\n",
        "    for entry in data:\n",
        "        q_type = entry['type']\n",
        "        if q_type not in question_types:\n",
        "            question_types[q_type] = []\n",
        "        question_types[q_type].append(entry)\n",
        "\n",
        "# Save each question type to separate JSON files\n",
        "output_files = []\n",
        "for q_type, questions in question_types.items():\n",
        "    filename = f\"/content/{q_type}.json\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(questions, f, indent=4)\n",
        "    output_files.append(filename)\n",
        "\n",
        "print(\"Files have been created.\")\n"
      ],
      "metadata": {
        "id": "qbMOX-eeOmDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON To String For Context"
      ],
      "metadata": {
        "id": "fzkGkBZ_VBfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load files\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "file_path_date = 'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/type_seperated/DATE.json'\n",
        "file_path_multi = 'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/type_seperated/MULTI_SELECT.json'\n",
        "file_path_number = 'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/type_seperated/NUMBER.json'\n",
        "file_path_single = 'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/type_seperated/SINGLE_SELECT.json'\n",
        "file_path_text = 'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/original_questionnaire/type_seperated/TEXT.json'\n",
        "\n",
        "# Function to load JSON from URL\n",
        "def load_json_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    return response.json()\n",
        "\n",
        "# Load JSON data from URLs\n",
        "try:\n",
        "    json_data_date = load_json_from_url(file_path_date)\n",
        "    json_data_multi = load_json_from_url(file_path_multi)\n",
        "    json_data_number = load_json_from_url(file_path_number)\n",
        "    json_data_single = load_json_from_url(file_path_single)\n",
        "    json_data_text = load_json_from_url(file_path_text)\n",
        "\n",
        "    # Convert JSON data to strings (if needed)\n",
        "    json_string_date = json.dumps(json_data_date, indent=4)\n",
        "    json_string_multi = json.dumps(json_data_multi, indent=4)\n",
        "    json_string_number = json.dumps(json_data_number, indent=4)\n",
        "    json_string_single = json.dumps(json_data_single, indent=4)\n",
        "    json_string_text = json.dumps(json_data_text, indent=4)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error loading JSON files: {e}\")\n"
      ],
      "metadata": {
        "id": "J63VsTV7dEyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Model"
      ],
      "metadata": {
        "id": "SWiAL7IBVM45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "\n",
        "multiVar = 10\n",
        "singleVar = 10\n",
        "numberVar = 50\n",
        "textVar = 50\n",
        "dateVar = 50\n",
        "\n",
        "prompt_multi = f\"\"\"\n",
        "                Using the context below as a template, create {multiVar} more JSON objects. The output should include only the keys \"type\",\n",
        "                \"question\", and \"options\". Ensure the \"type\" is always \"MULTI-SELECT\". The \"options\" array should contain multiple unique objects,\n",
        "                each with the \"option\" key and a meaningful value representing an available choice. No IDs should be included.\n",
        "                Generate multiple unique examples of questions that make sense in context for the type \"MULTI-SELECT\".\n",
        "                Output only valid JSON, with proper line spacing and indentation, without any additional formatting or code block delimiters.\n",
        "\n",
        "                Topic: Questionnnaire for evaluation. Diverse.\n",
        "\n",
        "                Context:\n",
        "                {json_string_multi}\n",
        "                \"\"\"\n",
        "\n",
        "prompt_single = f\"\"\"\n",
        "                Using the context below as a template, create {singleVar} more JSON objects. The output should include only the keys \"type\",\n",
        "                \"question\", and \"options\". Ensure the \"type\" is always \"SINGLE-SELECT\". The \"options\" array should contain multiple unique objects,\n",
        "                each with the \"option\" key and a meaningful value representing an available choice. No IDs should be included.\n",
        "                Generate multiple unique examples of questions that make sense in context for the type \"SINGLE-SELECT\".\n",
        "                Output only valid JSON, with proper line spacing and indentation, without any additional formatting or code block delimiters.\n",
        "\n",
        "                Topic: Questionnnaire for evaluation. Communications.\n",
        "\n",
        "                Context:\n",
        "                {json_string_single}\n",
        "                \"\"\"\n",
        "\n",
        "prompt_text = f\"\"\"\n",
        "                Using the context below as a template, create {textVar} more JSON objects. The output should include only the keys \"type\",\n",
        "                \"question\", and \"options\". Ensure the \"type\" is always \"TEXT\", the \"options\" array always contains exactly one object with\n",
        "                the \"option\" set to \"Text\", and no IDs are included. Generate multiple unique examples of questions that make sense in context\n",
        "                for the type \"TEXT\". They should be open ended. Output only valid JSON, with proper line spacing and indentation, without any additional formatting or code block delimiters.\n",
        "\n",
        "                Topic: Questionnnaire for evaluation. Communications. (General topic in the field of communications, not directly the word communications)\n",
        "\n",
        "                Context:\n",
        "                {json_string_text}\n",
        "                \"\"\"\n",
        "\n",
        "prompt_number = f\"\"\"\n",
        "                Using the context below as a template, create {numberVar} more JSON objects. The output should include only the keys \"type\",\n",
        "                \"question\", and \"options\". Ensure the \"type\" is always \"NUMBER\", the \"options\" array always contains exactly one object with\n",
        "                the \"option\" set to the category the question best, and no IDs are included. Generate multiple unique examples of questions that make sense in context\n",
        "                for the type \"NUMBER\". Output only valid JSON, with proper line spacing and indentation, without any additional formatting or code block delimiters.\n",
        "\n",
        "                Topic: Questionnnaire for evaluation. Convention/Fair.\n",
        "\n",
        "                Context:\n",
        "                {json_string_number}\n",
        "                \"\"\"\n",
        "\n",
        "prompt_date = f\"\"\"\n",
        "                Using the context below as a template, create {dateVar} more JSON objects. The output should include only the keys \"type\",\n",
        "                \"question\", and \"options\". Ensure the \"type\" is always \"DATE\", the \"options\" array always contains exactly one object with\n",
        "                the \"option\" set to \"Date\", and no IDs are included. Generate multiple unique examples of questions that make sense in context\n",
        "                for the type \"DATE\". Make sure the question specifically ask for a date. Add \"Provide a date.\" at the end of each question.\n",
        "                Output only valid JSON, with proper line spacing and indentation, without any additional formatting or code block delimiters.\n",
        "\n",
        "                Topic: Questionnnaire for evaluation. Healthcare.\n",
        "\n",
        "                Context:\n",
        "                {json_string_date}\n",
        "                \"\"\"\n",
        "\n",
        "\n",
        "# responses\n",
        "responses = {\n",
        "    #\"prompt_multi\": \"Questionnaire_Multi_Artificial.json\",\n",
        "    #\"prompt_single\": \"Questionnaire_Single_Artificial.json\",\n",
        "    #\"prompt_date\": \"Questionnaire_Date_Artificial.json\",\n",
        "    #\"prompt_number\": \"Questionnaire_Number_Artificial.json\",\n",
        "    \"prompt_text\": \"Questionnaire_Text_Artificial.json\"\n",
        "}\n",
        "\n",
        "for response_name, response_file in responses.items():\n",
        "    prompt = globals()[response_name]  # Get the prompt string\n",
        "    response = model.generate_content(prompt)  # Get model's response\n",
        "\n",
        "    try:\n",
        "        response_data = json.loads(response.text)  # Parse the response text as JSON\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON for {response_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Write the parsed JSON to a file\n",
        "    with open(response_file, \"w\") as json_file:\n",
        "        json.dump(response_data, json_file, indent=4)\n",
        "\n",
        "    print(f\"JSON file '{response_file}' has been created\")"
      ],
      "metadata": {
        "id": "sxSLA1-SbAG3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#JSON to Dataframe"
      ],
      "metadata": {
        "id": "xoc_3zsxUd6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MULTI"
      ],
      "metadata": {
        "id": "1E1TXueCdKw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "#Load the JSON data\n",
        "questions = [\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/MULTI/Questionnaire_Multi_Artificial_Diverse.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/MULTI/Questionnaire_Multi_Artificial_HeavyIndustry.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/MULTI/Questionnaire_Multi_Artificial_Sales.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/MULTI/Questionnaire_Multi_Artificial_SoftwareDev.json'\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for question in questions:\n",
        "    response = requests.get(question)\n",
        "    data = response.json()\n",
        "\n",
        "    #Create a DataFrame from the JSON data\n",
        "    df_temp = pd.DataFrame(data)\n",
        "    df_temp['options'] = df_temp['options'].apply(lambda x: [item['option'] for item in x])\n",
        "    dfs.append(df_temp)\n",
        "\n",
        "df_MULTI = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df_for_csv = df_MULTI.copy()\n",
        "df_for_csv['options'] = df_for_csv['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv.to_csv('MULTI_combined.csv', index=False)\n",
        "\n",
        "df_MULTI"
      ],
      "metadata": {
        "id": "harKmP4IUdgZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SINGLE"
      ],
      "metadata": {
        "id": "fF6N95AP2eJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "#Load the JSON data\n",
        "\n",
        "questions = [\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/SINGLE/Questionnaire_Single_Artificial_ArtIndustry.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/SINGLE/Questionnaire_Single_Artificial_Communications.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/SINGLE/Questionnaire_Single_Artificial_Diverse.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/main/artificial_questionnaire/SINGLE/Questionnaire_Single_Artificial_SoftwareDev.json'\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for question in questions:\n",
        "    response = requests.get(question)\n",
        "    data = response.json()\n",
        "\n",
        "    #Create a DataFrame from the JSON data\n",
        "\n",
        "    df_temp = pd.DataFrame(data)\n",
        "    df_temp['options'] = df_temp['options'].apply(lambda x: [item['option'] for item in x])\n",
        "    dfs.append(df_temp)\n",
        "\n",
        "df_SINGLE = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df_for_csv = df_SINGLE.copy()\n",
        "df_for_csv['options'] = df_for_csv['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv.to_csv('SINGLE_combined.csv', index=False)\n",
        "\n",
        "df_SINGLE\n"
      ],
      "metadata": {
        "id": "9fA2yR_k2f2Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NUMBER"
      ],
      "metadata": {
        "id": "_PF7TsoUGzVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "#Load the JSON data\n",
        "\n",
        "questions = [\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/NUMBER/Questionnaire_Number_Artificial_BusinessAndJob.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/NUMBER/Questionnaire_Number_Artificial_HeavyIndustries.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/NUMBER/Questionnaire_Number_Artificial_Diverse.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/NUMBER/Questionnaire_Number_Artificial_Customer.json'\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for question in questions:\n",
        "    response = requests.get(question)\n",
        "    data = response.json()\n",
        "\n",
        "    #Create a DataFrame from the JSON data\n",
        "\n",
        "    df_temp = pd.DataFrame(data)\n",
        "    df_temp['options'] = df_temp['options'].apply(lambda x: [item['option'] for item in x])\n",
        "    dfs.append(df_temp)\n",
        "\n",
        "df_NUMBER = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df_for_csv = df_NUMBER.copy()\n",
        "df_for_csv['options'] = df_for_csv['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv.to_csv('NUMBER_combined.csv', index=False)\n",
        "\n",
        "df_NUMBER"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ymm1w1W-FLfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATE"
      ],
      "metadata": {
        "id": "fknbAzNquMPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "#Load the JSON data\n",
        "\n",
        "questions = [\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/DATE/Questionnaire_Date_Artificial_Customer.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/DATE/Questionnaire_Date_Artificial_HeavyIndustries.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/DATE/Questionnaire_Date_Artificial_Technology.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/DATE/Questionnaire_Date_Artificial_Healthcare.json'\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for question in questions:\n",
        "    response = requests.get(question)\n",
        "    data = response.json()\n",
        "\n",
        "    #Create a DataFrame from the JSON data\n",
        "\n",
        "    df_temp = pd.DataFrame(data)\n",
        "    df_temp['options'] = df_temp['options'].apply(lambda x: [item['option'] for item in x])\n",
        "    dfs.append(df_temp)\n",
        "\n",
        "df_DATE = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df_for_csv = df_DATE.copy()\n",
        "df_for_csv['options'] = df_for_csv['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv.to_csv('DATE_combined.csv', index=False)\n",
        "\n",
        "df_DATE"
      ],
      "metadata": {
        "id": "pOMExV7LuSKa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEXT"
      ],
      "metadata": {
        "id": "7HtP3UXQa6zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "#Load the JSON data\n",
        "\n",
        "questions = [\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/TEXT/Questionnaire_Text_Artificial_Communications.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/TEXT/Questionnaire_Text_Artificial_HeavyIndustries.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/TEXT/Questionnaire_Text_Artificial_CustomerSupport.json',\n",
        "    'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_questionnaire/TEXT/Questionnaire_Text_Artificial_Convention.json'\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for question in questions:\n",
        "    response = requests.get(question)\n",
        "    data = response.json()\n",
        "\n",
        "    #Create a DataFrame from the JSON data\n",
        "\n",
        "    df_temp = pd.DataFrame(data)\n",
        "    df_temp['options'] = df_temp['options'].apply(lambda x: [item['option'] for item in x])\n",
        "    dfs.append(df_temp)\n",
        "\n",
        "df_TEXT = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "df_for_csv = df_TEXT.copy()\n",
        "df_for_csv['options'] = df_for_csv['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv.to_csv('TEXT_combined.csv', index=False)\n",
        "\n",
        "df_TEXT"
      ],
      "metadata": {
        "id": "PuXWkq5Ga8nr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Generation"
      ],
      "metadata": {
        "id": "jYs5pcRiHA45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MULTI"
      ],
      "metadata": {
        "id": "IJu2V5dNOlMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.api_core import exceptions\n",
        "\n",
        "df_MULTI_with_answers = df_MULTI.iloc[100:190].copy()\n",
        "\n",
        "# Generate a single compact prompt for all variations\n",
        "def generate_minimal_prompt(row):\n",
        "    return f\"\"\"Question: {row['question']}\n",
        "            Options: {', '.join(row['options'])}\n",
        "\n",
        "            Provide 5 different responses to this multiple choice question. Each response should select from the given options. Chose a different\n",
        "            cobination of options (or single option) for each response. Each answer (V, C, Q, E, A) is to be given by a seperate person.\n",
        "            Each label (V_Sel, C_Sel, Q_Sel, E_Sel, A_Sel) should contain the options corresponding to the response.\n",
        "            Format exactly as follows:\n",
        "            V: [verbose response, not exceeding 3 sentences]\n",
        "            V_Sel: [options corresponding to the response V]\n",
        "            C: [concise response]\n",
        "            C_Sel: [options corresponding to the response C]\n",
        "            Q: [colloquial response, no 'honestly']\n",
        "            Q_Sel: [options corresponding to the response Q]\n",
        "            E: [explanatory response, not exceeding 3 sentences]\n",
        "            E_Sel: [options corresponding to the response E]\n",
        "            A: [mildly annoyed response, no 'ugh']\n",
        "            A_Sel: [options corresponding to the response A]\"\"\"\n",
        "\n",
        "# Define the prefixes for each response type\n",
        "prefixes = {\n",
        "    'verbose': 'V:',\n",
        "    'verbose_Gemini_label': 'V_Sel: ',\n",
        "    'concise': 'C:',\n",
        "    'concise_Gemini_label': 'C_Sel: ',\n",
        "    'colloquial': 'Q:',\n",
        "    'colloquial_Gemini_label': 'Q_Sel: ',\n",
        "    'explanatory': 'E:',\n",
        "    'explanatory_Gemini_label': 'E_Sel: ',\n",
        "    'annoyed': 'A:',\n",
        "    'annoyed_Gemini_label': 'A_Sel: '\n",
        "}\n",
        "\n",
        "# Parse the response text into separate variations\n",
        "def parse_responses(text):\n",
        "    answers = {}\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Extract each response type\n",
        "    for key, prefix in prefixes.items():\n",
        "        try:\n",
        "            # Find the line starting with this prefix\n",
        "            response_line = next((line for line in lines if line.strip().startswith(prefix)), '')\n",
        "            # Remove the prefix and trim\n",
        "            response = response_line.replace(prefix, '', 1).strip()\n",
        "            answers[f\"answer_{key}\"] = response if response else f\"Error: No {key} response found\"\n",
        "        except Exception as e:\n",
        "            answers[f\"answer_{key}\"] = f\"Error parsing {key} response: {str(e)}\"\n",
        "\n",
        "    return answers\n",
        "\n",
        "# Process a single row with retries\n",
        "def process_row(row):\n",
        "    prompt = generate_minimal_prompt(row)\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            answers = parse_responses(response.text)\n",
        "            time.sleep(3)\n",
        "            return answers\n",
        "        except exceptions.TooManyRequests:\n",
        "            if attempt < 2:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return {f\"answer_{key}\": \"Error: Rate limit exceeded.\"\n",
        "                       for key in prefixes}\n",
        "        except Exception as e:\n",
        "            print(f\"Generated text was:\\n{response.text}\\n\")  # Print the response text\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            return {f\"answer_{key}\": f\"Error: {str(e)}\"\n",
        "                  for key in prefixes}\n",
        "\n",
        "# Process the dataframe\n",
        "answer_columns = df_MULTI_with_answers.apply(process_row, axis=1)\n",
        "\n",
        "# Add columns to dataframe\n",
        "for key in prefixes:\n",
        "    df_MULTI_with_answers[f\"answer_{key}\"] = answer_columns.apply(lambda x: x[f\"answer_{key}\"])\n",
        "\n",
        "# Save to CSV\n",
        "df_for_csv_MULTI_answer = df_MULTI_with_answers.copy()\n",
        "df_for_csv_MULTI_answer['options'] = df_for_csv_MULTI_answer['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv_MULTI_answer.to_csv('MULTI_answer_combined.csv', index=False, sep='>')\n",
        "\n",
        "df_MULTI_with_answers"
      ],
      "metadata": {
        "id": "7HnQEYdTcWJl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_0_100 = pd.read_csv('MULTI_answer_combined_0-100.csv', sep = '>')\n",
        "df_101_189 = pd.read_csv('MULTI_answer_combined_101-189.csv', sep = '>')\n",
        "\n",
        "df_MULTI_with_answers_total = pd.concat([df_0_100, df_101_189], ignore_index=True)\n",
        "df_MULTI_with_answers_total.to_csv('MULTI_answer_combined_total.csv', index=False, sep='>')\n"
      ],
      "metadata": {
        "id": "sZGCWwgNPZ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Melt the dataframe to combine all answer columns into one column\n",
        "df_long = df_MULTI_with_answers_total.melt(\n",
        "    id_vars=[\"type\", \"question\", \"options\"],\n",
        "    var_name=\"answer_type\",\n",
        "    value_name=\"answer_value\"\n",
        ")\n",
        "\n",
        "# Step 2: Create a new column for the answer type (e.g., verbose, concise, etc.)\n",
        "df_long[\"answer_category\"] = df_long[\"answer_type\"].str.extract(r\"answer_(\\w+)\")\n",
        "\n",
        "# Step 3: Separate rows into answers and their labels\n",
        "answers = df_long[~df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "labels = df_long[df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "\n",
        "# Step 4: Add a matching key column to facilitate merging\n",
        "answers[\"label_type\"] = answers[\"answer_type\"] + \"_Gemini_label\"\n",
        "\n",
        "# Step 5: Merge answers with their corresponding labels\n",
        "df_reshaped = pd.merge(\n",
        "    answers,\n",
        "    labels,\n",
        "    left_on=[\"type\", \"question\", \"options\", \"label_type\"],\n",
        "    right_on=[\"type\", \"question\", \"options\", \"answer_type\"],\n",
        "    suffixes=(\"\", \"_label\")\n",
        ")\n",
        "\n",
        "# Step 6: Rename columns and keep the desired ones\n",
        "df_reshaped = df_reshaped.rename(\n",
        "    columns={\"answer_value\": \"answer\", \"answer_value_label\": \"answer_label_GEMINI\"}\n",
        ")[[\"type\", \"question\", \"options\", \"answer\", \"answer_label_GEMINI\", \"answer_category\"]]\n",
        "\n",
        "# Step 7: Sort the data if needed (optional)\n",
        "df_reshaped_MULTI = df_reshaped.sort_values(by=[\"type\", \"question\"]).reset_index(drop=True)\n",
        "\n",
        "# Display the final dataframe\n",
        "df_reshaped_MULTI.to_csv('MULTI_answer_combined_total_reshaped.csv', index=False, sep='>')\n",
        "df_reshaped_MULTI\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hmW2i6BgUohp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SINGLE"
      ],
      "metadata": {
        "id": "EjMmZuD9gntG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.api_core import exceptions\n",
        "\n",
        "df_SINGLE_with_answers = df_SINGLE.iloc[101:150].copy()\n",
        "\n",
        "def generate_minimal_prompt(row):\n",
        "    return f\"\"\"Question: {row['question']}\n",
        "            Options: {', '.join(row['options'])}\n",
        "            Provide 5 different responses to this single choice question. Each response should select from the given options. Chose a different\n",
        "            option for each response (if enough options exist for all 5 responses, otherwise repeat options after having already used all of them).\n",
        "            Each answer (V, C, Q, E, A) is to be given by a seperate person.\n",
        "            Each label (V_Sel, C_Sel, Q_Sel, E_Sel, A_Sel) should contain the option corresponding to the response. Format exactly as follows:\n",
        "            V: [verbose response, not exceeding 3 sentences]\n",
        "            V_Sel: [option corresponding to the response V]\n",
        "            C: [concise response]\n",
        "            C_Sel: [option corresponding to the response C]\n",
        "            Q: [colloquial response, no 'honestly']\n",
        "            Q_Sel: [option corresponding to the response Q]\n",
        "            E: [explanatory response, not exceeding 3 sentences]\n",
        "            E_Sel: [option corresponding to the response E]\n",
        "            A: [mildly annoyed response, no 'ugh']\n",
        "            A_Sel: [option corresponding to the response A]\"\"\"\n",
        "\n",
        "# Define the prefixes for each response type\n",
        "prefixes = {\n",
        "    'verbose': 'V:',\n",
        "    'verbose_Gemini_label': 'V_Sel: ',\n",
        "    'concise': 'C:',\n",
        "    'concise_Gemini_label': 'C_Sel: ',\n",
        "    'colloquial': 'Q:',\n",
        "    'colloquial_Gemini_label': 'Q_Sel: ',\n",
        "    'explanatory': 'E:',\n",
        "    'explanatory_Gemini_label': 'E_Sel: ',\n",
        "    'annoyed': 'A:',\n",
        "    'annoyed_Gemini_label': 'A_Sel: '\n",
        "}\n",
        "\n",
        "def parse_responses(text):\n",
        "    answers = {}\n",
        "    lines = text.split('\\n')\n",
        "    for key, prefix in prefixes.items():\n",
        "        try:\n",
        "            for line in lines:\n",
        "                if line.startswith(prefix):\n",
        "                    line = line.removeprefix(prefix)\n",
        "                    answers[f\"answer_{key}\"] = line if line else f\"Error: No {key} response found\"\n",
        "        except Exception as e:\n",
        "                answers[f\"answer_{key}\"] = f\"Error parsing {key} response: {str(e)}\"\n",
        "\n",
        "    return answers\n",
        "\n",
        "def process_row(row):\n",
        "    prompt = generate_minimal_prompt(row)\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            answers = parse_responses(response.text)\n",
        "            time.sleep(3)\n",
        "            return answers\n",
        "        except exceptions.TooManyRequests:\n",
        "            if attempt < 2:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return {f\"answer_{key}\": \"Error: Rate limit exceeded.\"\n",
        "                       for key in prefixes}\n",
        "        except Exception as e:\n",
        "            print(f\"Generated text was:\\n{response.text}\\n\")  # Print the response text\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            return {f\"answer_{key}\": f\"Error: {str(e)}\"\n",
        "                  for key in prefixes}\n",
        "\n",
        "answer_columns = df_SINGLE_with_answers.apply(process_row, axis=1)\n",
        "\n",
        "# Add columns to dataframe\n",
        "for key in prefixes:\n",
        "    df_SINGLE_with_answers[f\"answer_{key}\"] = answer_columns.apply(lambda x: x[f\"answer_{key}\"])\n",
        "\n",
        "df_for_csv_SINGLE_answer = df_SINGLE_with_answers.copy()\n",
        "df_for_csv_SINGLE_answer['options'] = df_for_csv_SINGLE_answer['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv_SINGLE_answer.to_csv('SINGLE_answer_combined.csv', index=False, sep='>')\n",
        "\n",
        "df_SINGLE_with_answers"
      ],
      "metadata": {
        "id": "1lfnfIlMKer7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_0_100 = pd.read_csv('SINGLE_answer_combined_0-100.csv', sep = '>')\n",
        "df_101_189 = pd.read_csv('SINGLE_answer_combined_101-149.csv', sep = '>')\n",
        "\n",
        "df_SINGLE_with_answers_total = pd.concat([df_0_100, df_101_189], ignore_index=True)\n",
        "df_SINGLE_with_answers_total.to_csv('SINGLE_answer_combined_total.csv', index=False, sep='>')"
      ],
      "metadata": {
        "id": "GHnb9QaWWIj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Melt the dataframe to combine all answer columns into one column\n",
        "df_long = df_SINGLE_with_answers_total.melt(\n",
        "    id_vars=[\"type\", \"question\", \"options\"],\n",
        "    var_name=\"answer_type\",\n",
        "    value_name=\"answer_value\"\n",
        ")\n",
        "\n",
        "# Step 2: Create a new column for the answer type (e.g., verbose, concise, etc.)\n",
        "df_long[\"answer_category\"] = df_long[\"answer_type\"].str.extract(r\"answer_(\\w+)\")\n",
        "\n",
        "# Step 3: Separate rows into answers and their labels\n",
        "answers = df_long[~df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "labels = df_long[df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "\n",
        "# Step 4: Add a matching key column to facilitate merging\n",
        "answers[\"label_type\"] = answers[\"answer_type\"] + \"_Gemini_label\"\n",
        "\n",
        "# Step 5: Merge answers with their corresponding labels\n",
        "df_reshaped = pd.merge(\n",
        "    answers,\n",
        "    labels,\n",
        "    left_on=[\"type\", \"question\", \"options\", \"label_type\"],\n",
        "    right_on=[\"type\", \"question\", \"options\", \"answer_type\"],\n",
        "    suffixes=(\"\", \"_label\")\n",
        ")\n",
        "\n",
        "# Step 6: Rename columns and keep the desired ones\n",
        "df_reshaped = df_reshaped.rename(\n",
        "    columns={\"answer_value\": \"answer\", \"answer_value_label\": \"answer_label_GEMINI\"}\n",
        ")[[\"type\", \"question\", \"options\", \"answer\", \"answer_label_GEMINI\", \"answer_category\"]]\n",
        "\n",
        "# Step 7: Sort the data if needed (optional)\n",
        "df_reshaped_SINGLE = df_reshaped.sort_values(by=[\"type\", \"question\"]).reset_index(drop=True)\n",
        "\n",
        "# Display the final dataframe\n",
        "df_reshaped_SINGLE.to_csv('SINGLE_answer_combined_total_reshaped.csv', index=False, sep='>')\n",
        "df_reshaped_SINGLE"
      ],
      "metadata": {
        "collapsed": true,
        "id": "58o5474EWVXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NUMBER"
      ],
      "metadata": {
        "id": "0ise3ICagtyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.api_core import exceptions\n",
        "\n",
        "df_NUMBER_with_answers = df_NUMBER.copy()\n",
        "\n",
        "def generate_minimal_prompt(row):\n",
        "    return f\"\"\"Question: {row['question']}\n",
        "            Options: {row['options']}\n",
        "            Provide 5 different responses to this single choice question, which asks for a number. The question provides a single option defining the type of number to be chosen.\n",
        "            Each response should adhere to the given type but provide a unique answer. Some answers should be written as numbers, some such that the numbers are spelled out.\n",
        "            Each answer (V, C, Q, E, A) is to be given by a separate person.\n",
        "            Each label (V_Sel, C_Sel, Q_Sel, E_Sel, A_Sel) should contain just the number (with correct units) corresponding to the response. Format exactly as follows:\n",
        "            V: [verbose response, not exceeding 3 sentences]\n",
        "            V_Sel: [number corresponding to the response V]\n",
        "            C: [concise response]\n",
        "            C_Sel: [number corresponding to the response C]\n",
        "            Q: [colloquial response, no 'honestly']\n",
        "            Q_Sel: [number corresponding to the response Q]\n",
        "            E: [explanatory response, not exceeding 3 sentences]\n",
        "            E_Sel: [number corresponding to the response E]\n",
        "            A: [mildly annoyed response, no 'ugh']\n",
        "            A_Sel: [number corresponding to the response A]\"\"\"\n",
        "\n",
        "# Define the prefixes for each response type\n",
        "prefixes = {\n",
        "    'verbose': 'V:',\n",
        "    'verbose_Gemini_label': 'V_Sel: ',\n",
        "    'concise': 'C:',\n",
        "    'concise_Gemini_label': 'C_Sel: ',\n",
        "    'colloquial': 'Q:',\n",
        "    'colloquial_Gemini_label': 'Q_Sel: ',\n",
        "    'explanatory': 'E:',\n",
        "    'explanatory_Gemini_label': 'E_Sel: ',\n",
        "    'annoyed': 'A:',\n",
        "    'annoyed_Gemini_label': 'A_Sel: '\n",
        "}\n",
        "\n",
        "def parse_responses(text):\n",
        "    answers = {}\n",
        "    lines = text.split('\\n')\n",
        "    for key, prefix in prefixes.items():\n",
        "        try:\n",
        "            for line in lines:\n",
        "                if line.startswith(prefix):\n",
        "                    line = line.removeprefix(prefix)\n",
        "                    answers[f\"answer_{key}\"] = line if line else f\"Error: No {key} response found\"\n",
        "        except Exception as e:\n",
        "                answers[f\"answer_{key}\"] = f\"Error parsing {key} response: {str(e)}\"\n",
        "\n",
        "    return answers\n",
        "\n",
        "def process_row(row):\n",
        "    prompt = generate_minimal_prompt(row)\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            answers = parse_responses(response.text)\n",
        "            time.sleep(3)\n",
        "            return answers\n",
        "        except exceptions.TooManyRequests:\n",
        "            if attempt < 2:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return {f\"answer_{key}\": \"Error: Rate limit exceeded.\"\n",
        "                       for key in prefixes}\n",
        "        except Exception as e:\n",
        "            print(f\"Generated text was:\\n{response.text}\\n\")  # Print the response text\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            return {f\"answer_{key}\": f\"Error: {str(e)}\"\n",
        "                  for key in prefixes}\n",
        "\n",
        "answer_columns = df_NUMBER_with_answers.apply(process_row, axis=1)\n",
        "\n",
        "for key in prefixes:\n",
        "    df_NUMBER_with_answers[f\"answer_{key}\"] = answer_columns.apply(lambda x: x[f\"answer_{key}\"])\n",
        "\n",
        "df_for_csv_NUMBER_answer = df_NUMBER_with_answers.copy()\n",
        "df_for_csv_NUMBER_answer['options'] = df_for_csv_NUMBER_answer['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv_NUMBER_answer.to_csv('NUMBER_answer_combined.csv', index=False, sep='>')\n",
        "\n",
        "df_NUMBER_with_answers"
      ],
      "metadata": {
        "id": "aU9ROXGkgtcP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert options to strings before melting\n",
        "df_NUMBER_with_answers['options'] = df_NUMBER_with_answers['options'].astype(str)\n",
        "\n",
        "# Step 1: Melt the dataframe to combine all answer columns into one column\n",
        "df_long = df_NUMBER_with_answers.melt(\n",
        "    id_vars=[\"type\", \"question\", \"options\"],\n",
        "    var_name=\"answer_type\",\n",
        "    value_name=\"answer_value\"\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Create a new column for the answer type (e.g., verbose, concise, etc.)\n",
        "df_long[\"answer_category\"] = df_long[\"answer_type\"].str.extract(r\"answer_(\\w+)\")\n",
        "\n",
        "# Step 3: Separate rows into answers and their labels\n",
        "answers = df_long[~df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "labels = df_long[df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "\n",
        "# Step 4: Add a matching key column to facilitate merging\n",
        "answers[\"label_type\"] = answers[\"answer_type\"] + \"_Gemini_label\"\n",
        "\n",
        "# Step 5: Merge answers with their corresponding labels\n",
        "df_reshaped = pd.merge(\n",
        "    answers,\n",
        "    labels,\n",
        "    left_on=[\"type\", \"question\", \"options\", \"label_type\"],\n",
        "    right_on=[\"type\", \"question\", \"options\", \"answer_type\"],\n",
        "    suffixes=(\"\", \"_label\")\n",
        ")\n",
        "\n",
        "# Step 6: Rename columns and keep the desired ones\n",
        "df_reshaped = df_reshaped.rename(\n",
        "    columns={\"answer_value\": \"answer\", \"answer_value_label\": \"answer_label_GEMINI\"}\n",
        ")[[\"type\", \"question\", \"options\", \"answer\", \"answer_label_GEMINI\", \"answer_category\"]]\n",
        "\n",
        "# Step 7: Sort the data if needed (optional)\n",
        "df_reshaped_NUMBER = df_reshaped.sort_values(by=[\"type\", \"question\"]).reset_index(drop=True)\n",
        "\n",
        "# Display the final dataframe\n",
        "df_reshaped_NUMBER.to_csv('NUMBER_answer_combined_total_reshaped.csv', index=False, sep='>')\n",
        "df_reshaped_NUMBER"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rYk5P-mpdlb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATE"
      ],
      "metadata": {
        "id": "ECZEWIdvu1c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.api_core import exceptions\n",
        "\n",
        "df_DATE_with_answers = df_DATE.copy()\n",
        "\n",
        "def generate_minimal_prompt(row):\n",
        "    return f\"\"\"Question: {row['question']}\n",
        "            Options: {row['options']}\n",
        "            Provide 5 different responses to this single choice question, which asks for a date. The question provides one option defining the type to be chosen.\n",
        "            Each response should adhere to the given type but provide a unique answer. Some answers should be written as numbers, some such that the numbers are spelled out. Use different date formats.\n",
        "            Assume question takers are from Europe.\n",
        "            Each answer (V, C, Q, E, A) is to be given by a separate person.\n",
        "            Each label (V_Sel, C_Sel, Q_Sel, E_Sel, A_Sel) should contain just the date corresponding to the response. Use this date format: dd-mm-yyyy and nothing else for the label.\n",
        "            Format exactly as follows:\n",
        "            V: [verbose response, not exceeding 3 sentences]\n",
        "            V_Sel: [date corresponding to the response V]\n",
        "            C: [concise response]\n",
        "            C_Sel: [date corresponding to the response C]\n",
        "            Q: [colloquial response, no 'honestly']\n",
        "            Q_Sel: [date corresponding to the response Q]\n",
        "            E: [explanatory response, not exceeding 3 sentences]\n",
        "            E_Sel: [date corresponding to the response E]\n",
        "            A: [mildly annoyed response, no 'ugh']\n",
        "            A_Sel: [date corresponding to the response A]\"\"\"\n",
        "\n",
        "# Define the prefixes for each response type\n",
        "prefixes = {\n",
        "    'verbose': 'V:',\n",
        "    'verbose_Gemini_label': 'V_Sel: ',\n",
        "    'concise': 'C:',\n",
        "    'concise_Gemini_label': 'C_Sel: ',\n",
        "    'colloquial': 'Q:',\n",
        "    'colloquial_Gemini_label': 'Q_Sel: ',\n",
        "    'explanatory': 'E:',\n",
        "    'explanatory_Gemini_label': 'E_Sel: ',\n",
        "    'annoyed': 'A:',\n",
        "    'annoyed_Gemini_label': 'A_Sel: '\n",
        "}\n",
        "\n",
        "def parse_responses(text):\n",
        "    answers = {}\n",
        "    lines = text.split('\\n')\n",
        "    for key, prefix in prefixes.items():\n",
        "        try:\n",
        "            for line in lines:\n",
        "                if line.startswith(prefix):\n",
        "                    line = line.removeprefix(prefix)\n",
        "                    answers[f\"answer_{key}\"] = line if line else f\"Error: No {key} response found\"\n",
        "        except Exception as e:\n",
        "                answers[f\"answer_{key}\"] = f\"Error parsing {key} response: {str(e)}\"\n",
        "\n",
        "    return answers\n",
        "\n",
        "def process_row(row):\n",
        "    prompt = generate_minimal_prompt(row)\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            answers = parse_responses(response.text)\n",
        "            time.sleep(3)\n",
        "            return answers\n",
        "        except exceptions.TooManyRequests:\n",
        "            if attempt < 2:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return {f\"answer_{key}\": \"Error: Rate limit exceeded.\"\n",
        "                       for key in prefixes}\n",
        "        except Exception as e:\n",
        "            print(f\"Generated text was:\\n{response.text}\\n\")  # Print the response text\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            return {f\"answer_{key}\": f\"Error: {str(e)}\"\n",
        "                  for key in prefixes}\n",
        "\n",
        "answer_columns = df_DATE_with_answers.apply(process_row, axis=1)\n",
        "\n",
        "for key in prefixes:\n",
        "    df_DATE_with_answers[f\"answer_{key}\"] = answer_columns.apply(lambda x: x[f\"answer_{key}\"])\n",
        "\n",
        "df_for_csv_DATE_answer = df_DATE_with_answers.copy()\n",
        "df_for_csv_DATE_answer['options'] = df_for_csv_DATE_answer['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv_DATE_answer.to_csv('DATE_answer_combined.csv', index=False, sep='>')\n",
        "\n",
        "df_DATE_with_answers"
      ],
      "metadata": {
        "id": "iXfSbksBu5ZP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert options to strings before melting\n",
        "df_DATE_with_answers['options'] = df_DATE_with_answers['options'].astype(str)\n",
        "\n",
        "# Step 1: Melt the dataframe to combine all answer columns into one column\n",
        "df_long = df_DATE_with_answers.melt(\n",
        "    id_vars=[\"type\", \"question\", \"options\"],\n",
        "    var_name=\"answer_type\",\n",
        "    value_name=\"answer_value\"\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Create a new column for the answer type (e.g., verbose, concise, etc.)\n",
        "df_long[\"answer_category\"] = df_long[\"answer_type\"].str.extract(r\"answer_(\\w+)\")\n",
        "\n",
        "# Step 3: Separate rows into answers and their labels\n",
        "answers = df_long[~df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "labels = df_long[df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "\n",
        "# Step 4: Add a matching key column to facilitate merging\n",
        "answers[\"label_type\"] = answers[\"answer_type\"] + \"_Gemini_label\"\n",
        "\n",
        "# Step 5: Merge answers with their corresponding labels\n",
        "df_reshaped = pd.merge(\n",
        "    answers,\n",
        "    labels,\n",
        "    left_on=[\"type\", \"question\", \"options\", \"label_type\"],\n",
        "    right_on=[\"type\", \"question\", \"options\", \"answer_type\"],\n",
        "    suffixes=(\"\", \"_label\")\n",
        ")\n",
        "\n",
        "# Step 6: Rename columns and keep the desired ones\n",
        "df_reshaped = df_reshaped.rename(\n",
        "    columns={\"answer_value\": \"answer\", \"answer_value_label\": \"answer_label_GEMINI\"}\n",
        ")[[\"type\", \"question\", \"options\", \"answer\", \"answer_label_GEMINI\", \"answer_category\"]]\n",
        "\n",
        "# Step 7: Sort the data if needed (optional)\n",
        "df_reshaped_DATE = df_reshaped.sort_values(by=[\"type\", \"question\"]).reset_index(drop=True)\n",
        "\n",
        "# Display the final dataframe\n",
        "df_reshaped_DATE.to_csv('DATE_answer_combined_total_reshaped.csv', index=False, sep='>')\n",
        "df_reshaped_DATE"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sdK6AoV2kX6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEXT"
      ],
      "metadata": {
        "id": "eo-6wzxObq_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.api_core import exceptions\n",
        "\n",
        "df_TEXT_with_answers = df_TEXT.copy()\n",
        "\n",
        "def generate_minimal_prompt(row):\n",
        "    return f\"\"\"Question: {row['question']}\n",
        "            Options: {row['options']}\n",
        "            Provide 5 different responses to this single choice question, which asks for a text. The question provides a single option defining the type of answer to be chosen.\n",
        "            Each response should adhere to the given type but provide a unique answer. The questions are open ended, therfore your answer should replicate a human's answer to such a question.\n",
        "            Each answer (V, C, Q, E, A) is to be given by a separate person.\n",
        "            Each label (V_Sel, C_Sel, Q_Sel, E_Sel, A_Sel) should contain the shortest possible topic summery.\n",
        "            Format exactly as follows:\n",
        "            V: [verbose response, not exceeding 3 sentences]\n",
        "            V_Sel: [summery corresponding to the response V]\n",
        "            C: [concise response]\n",
        "            C_Sel: [summery corresponding to the response C]\n",
        "            Q: [colloquial response, no 'honestly']\n",
        "            Q_Sel: [summery corresponding to the response Q]\n",
        "            E: [explanatory response, not exceeding 3 sentences]\n",
        "            E_Sel: [summery corresponding to the response E]\n",
        "            A: [mildly annoyed response, no 'ugh']\n",
        "            A_Sel: [summery corresponding to the response A]\"\"\"\n",
        "\n",
        "# Define the prefixes for each response type\n",
        "prefixes = {\n",
        "    'verbose': 'V:',\n",
        "    'verbose_Gemini_label': 'V_Sel: ',\n",
        "    'concise': 'C:',\n",
        "    'concise_Gemini_label': 'C_Sel: ',\n",
        "    'colloquial': 'Q:',\n",
        "    'colloquial_Gemini_label': 'Q_Sel: ',\n",
        "    'explanatory': 'E:',\n",
        "    'explanatory_Gemini_label': 'E_Sel: ',\n",
        "    'annoyed': 'A:',\n",
        "    'annoyed_Gemini_label': 'A_Sel: '\n",
        "}\n",
        "\n",
        "def parse_responses(text):\n",
        "    answers = {}\n",
        "    lines = text.split('\\n')\n",
        "    for key, prefix in prefixes.items():\n",
        "        try:\n",
        "            for line in lines:\n",
        "                if line.startswith(prefix):\n",
        "                    line = line.removeprefix(prefix)\n",
        "                    answers[f\"answer_{key}\"] = line if line else f\"Error: No {key} response found\"\n",
        "        except Exception as e:\n",
        "                answers[f\"answer_{key}\"] = f\"Error parsing {key} response: {str(e)}\"\n",
        "\n",
        "    return answers\n",
        "\n",
        "def process_row(row):\n",
        "    prompt = generate_minimal_prompt(row)\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            answers = parse_responses(response.text)\n",
        "            time.sleep(3)\n",
        "            return answers\n",
        "        except exceptions.TooManyRequests:\n",
        "            if attempt < 2:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return {f\"answer_{key}\": \"Error: Rate limit exceeded.\"\n",
        "                       for key in prefixes}\n",
        "        except Exception as e:\n",
        "            print(f\"Generated text was:\\n{response.text}\\n\")  # Print the response text\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            return {f\"answer_{key}\": f\"Error: {str(e)}\"\n",
        "                  for key in prefixes}\n",
        "\n",
        "answer_columns = df_TEXT_with_answers.apply(process_row, axis=1)\n",
        "\n",
        "for key in prefixes:\n",
        "    df_TEXT_with_answers[f\"answer_{key}\"] = answer_columns.apply(lambda x: x[f\"answer_{key}\"])\n",
        "\n",
        "df_for_csv_TEXT_answer = df_TEXT_with_answers.copy()\n",
        "df_for_csv_TEXT_answer['options'] = df_for_csv_TEXT_answer['options'].apply(lambda x: '; '.join(x))\n",
        "df_for_csv_TEXT_answer.to_csv('TEXT_answer_combined.csv', index=False, sep='>')\n",
        "\n",
        "df_TEXT_with_answers"
      ],
      "metadata": {
        "id": "gHv02WL_bsZ7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert options to strings before melting\n",
        "df_TEXT_with_answers['options'] = df_TEXT_with_answers['options'].astype(str)\n",
        "\n",
        "# Step 1: Melt the dataframe to combine all answer columns into one column\n",
        "df_long = df_TEXT_with_answers.melt(\n",
        "    id_vars=[\"type\", \"question\", \"options\"],\n",
        "    var_name=\"answer_type\",\n",
        "    value_name=\"answer_value\"\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Create a new column for the answer type (e.g., verbose, concise, etc.)\n",
        "df_long[\"answer_category\"] = df_long[\"answer_type\"].str.extract(r\"answer_(\\w+)\")\n",
        "\n",
        "# Step 3: Separate rows into answers and their labels\n",
        "answers = df_long[~df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "labels = df_long[df_long[\"answer_type\"].str.endswith(\"_Gemini_label\")].copy()\n",
        "\n",
        "# Step 4: Add a matching key column to facilitate merging\n",
        "answers[\"label_type\"] = answers[\"answer_type\"] + \"_Gemini_label\"\n",
        "\n",
        "# Step 5: Merge answers with their corresponding labels\n",
        "df_reshaped = pd.merge(\n",
        "    answers,\n",
        "    labels,\n",
        "    left_on=[\"type\", \"question\", \"options\", \"label_type\"],\n",
        "    right_on=[\"type\", \"question\", \"options\", \"answer_type\"],\n",
        "    suffixes=(\"\", \"_label\")\n",
        ")\n",
        "\n",
        "# Step 6: Rename columns and keep the desired ones\n",
        "df_reshaped = df_reshaped.rename(\n",
        "    columns={\"answer_value\": \"answer\", \"answer_value_label\": \"answer_label_GEMINI\"}\n",
        ")[[\"type\", \"question\", \"options\", \"answer\", \"answer_label_GEMINI\", \"answer_category\"]]\n",
        "\n",
        "# Step 7: Sort the data if needed (optional)\n",
        "df_reshaped_TEXT = df_reshaped.sort_values(by=[\"type\", \"question\"]).reset_index(drop=True)\n",
        "\n",
        "# Display the final dataframe\n",
        "df_reshaped_TEXT.to_csv('TEXT_answer_combined_total_reshaped.csv', index=False, sep='>')\n",
        "df_reshaped_TEXT"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8oh-H7Ppkjy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge all CSV Files"
      ],
      "metadata": {
        "id": "fpllBNgP4lnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# list all csv files only\n",
        "csv_files = ['TEXT_answer_combined_total_reshaped.csv', 'DATE_answer_combined_total_reshaped.csv', 'NUMBER_answer_combined_total_reshaped.csv', 'SINGLE_answer_combined_total_reshaped.csv', 'MULTI_answer_combined_total_reshaped.csv']\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty list to store each DataFrame\n",
        "dfs = []\n",
        "\n",
        "# Read each CSV file and append to the list\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file, sep='>')\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Save the merged DataFrame to a new CSV\n",
        "merged_df.to_csv('all_answers_combined_reshaped.csv', index=False, sep='>')\n"
      ],
      "metadata": {
        "id": "3gPayKx5r-MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.read_csv('all_answers_combined_reshaped.csv', sep='>')\n",
        "df_all"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EL7_TQMzsorm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['options'] = df_all['options'].str.replace(r\"\\['|'\\]\", \"\", regex=True)"
      ],
      "metadata": {
        "id": "uR_rbnNdvV18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.to_csv('all_answers_combined_reshaped.csv', index=False, sep='>')"
      ],
      "metadata": {
        "id": "86ZLZ9PYvgg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Up CSV"
      ],
      "metadata": {
        "id": "27qOi2nti_bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all rows containing quotation marks\n",
        "\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def clean_csv_from_github(url, output_file):\n",
        "    \"\"\"\n",
        "    Reads a CSV file from a GitHub raw URL, removes rows containing quotation marks,\n",
        "    and saves the cleaned data to a new CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The GitHub raw URL of the CSV file.\n",
        "        output_file (str): The file path to save the cleaned CSV.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Fetch the CSV content from the GitHub raw URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "\n",
        "        # Decode the content and split into lines\n",
        "        csv_content = response.text\n",
        "        csv_lines = csv_content.splitlines()\n",
        "\n",
        "        # Read the CSV into memory\n",
        "        reader = csv.reader(csv_lines, delimiter='>')\n",
        "        cleaned_rows = []\n",
        "\n",
        "        for row in reader:\n",
        "            # Remove rows containing quotation marks\n",
        "            if not any('\"' in field for field in row):\n",
        "                cleaned_rows.append(row)\n",
        "\n",
        "        # Write the cleaned rows to a new CSV file\n",
        "        with open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
        "            writer = csv.writer(outfile, delimiter='>')\n",
        "            writer.writerows(cleaned_rows)\n",
        "\n",
        "        print(f\"Cleaned CSV saved to {output_file}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the CSV file: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_answers/all_answers_combined_reshaped.csv'\n",
        "output_file = 'cleaned_all_answers_combined_reshaped.csv'\n",
        "clean_csv_from_github(url, output_file)"
      ],
      "metadata": {
        "id": "YyZdvoVCjDsc",
        "outputId": "bc231184-056f-470f-acd8-d2d0be92e76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned CSV saved to cleaned_all_answers_combined_reshaped.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Additional Labels"
      ],
      "metadata": {
        "id": "G8MTiEvrki5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "\n",
        "def load_csv_method1(url):\n",
        "    df = pd.read_csv(url, sep='>', names=['type', 'question', 'options', 'answer', 'answer_label_GEMINI', 'answer_category'], converters={'options': lambda x: x.split(';')})\n",
        "    return df\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Penguinbeanie/Capstone-Project/refs/heads/dev_branch/artificial_answers/cleaned_all_answers_combined_reshaped.csv'\n",
        "df_full = load_csv_method1(url)\n",
        "df_MULTI_full = df_full[df_full['type'] == 'MULTI-SELECT']\n",
        "df_MULTI = df_MULTI_full.copy()\n",
        "\n",
        "print(df_MULTI)\n",
        "\n",
        "\n",
        "# Function to classify each row\n",
        "def classify_row(row):\n",
        "    result = classifier(row[\"answer\"], candidate_labels=row[\"options\"], multi_label=True)\n",
        "    result_list = list(zip(result[\"labels\"], result[\"scores\"]))\n",
        "    filtered_list = [label for label, score in result_list if score > 0.5]\n",
        "    print(filtered_list)\n",
        "\n",
        "    return filtered_list\n",
        "\n",
        "\n",
        "# Apply the classification to each row\n",
        "\n",
        "df_MULTI[\"answer_label_BART_large_mnli\"] = df_MULTI.apply(lambda row: classify_row(row), axis=1)\n",
        "\n",
        "# Display the DataFrame\n",
        "df_MULTI\n",
        "\n",
        "# drop all rows where type == MULTI-SELECT\n",
        "df_full = df_full.drop(df_full[df_full['type'] == 'MULTI-SELECT'].index)\n",
        "\n",
        "# append df_MULTI to df_full\n",
        "df_full = pd.concat([df_full, df_MULTI])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "32dde54f17f1456eb4d2fc20668c7c24",
            "1b4732b0e4fe4153af55acb8d76bde92",
            "6c500b72b1d645d4a2fc7b034adff57f",
            "8d293aa7f918422596e95e5026f7404d",
            "aa59f9aefba04965a1fe5bd9ab8e5274",
            "a917f9082b8749549383fd68bd81a6f1",
            "bf2bc25df11349ae8cdd704b02dab5a6",
            "2bace8ecf2c84054a86dbce93136712f",
            "cceb7827f7454d61affd260489e85954",
            "cfd3712ce42b4bf99604ff0c453277c5",
            "3b771b508cee4bc58530183e3986cb3b",
            "2be75242495e42618a770fec255199c4",
            "34b99293ee3647048f8c19d9e12a822a",
            "61f89768909f482ab193aa768eaf8e8d",
            "1c3fc40e67b3441d8ca2005864d3711f",
            "c3f83efcb20145439810844e3fe0f3de",
            "dc0e8fb98d3d4427bded5e29b3558029",
            "85875d0eb2524e16ad7a81fdb519f65d",
            "19051863e0a54d62b0cfd0e3cd94e934",
            "510d26b2e0d64832ba01044e93bfe3d5",
            "788c3515c0aa4dc5b4c3fcd805e36a9d",
            "e3e02c0093364e70a55062b6ee8ef8a0",
            "6b20644889614d9597883b6c7ed7c4b0",
            "066e674fdac44fa29c6895fc955c3e3d",
            "23ab41fe61fe4442ae33de4537e038f6",
            "ba7d60018ec74fe39c5a4b7437ae4217",
            "86eab91cc6124b4e9dacdd52897e2fa6",
            "3fa8fdde1a614c6090944b0e770d7db4",
            "e752cddbb17c46c880d097218d270288",
            "ce36a408a4054a17a870bdc4b273536a",
            "42161ed80d13494f91a66e6667b0a525",
            "a002661a939643f2bc3c36ba43ff8627",
            "428f9c7290e2400890771d9993fdd3db",
            "528af380945445aba679e635bfd113c4",
            "ebf6bbb4ecf5436b9a564c6e436c8e09",
            "82d81624f5b44b7eac46e99ea1bebc32",
            "5610a4cbb2564634bc05c61476c7df2c",
            "98be7b9e58074c8fb2d97213bea9fec0",
            "000221914f414d01895e767eb84c411e",
            "6cff377157e146558fb2296733c47865",
            "9772fca83ded4aedbe296f051253dd19",
            "d5fa67e0a84944a08d35b3acf8d86b9b",
            "d06115dad61a47d4a41c7ae38a8bfc4a",
            "1c97a00d8e7442349f313354611f902e",
            "d947e3eac0164714a74011f35d0af3c7",
            "36ff5c0fa7d84845b5fef1873c825d3a",
            "a7b1cf12477244e3b07d0f01c64e5bb6",
            "198633380b344515af498b648ccbc189",
            "7812dd0f38a74993af46b9764cf27e5e",
            "6ef66afd03734aba8de28d8e577a3413",
            "8ac6506e615144739d4a3094fc339ac5",
            "ceedbd90ed884e47b9957ad39b6759ec",
            "b22906a7e6cb4b80850218e0ba37aab1",
            "33d92e6a03894ac286fac2543f938822",
            "57a30954650949c4b5eb3572c1032c94",
            "be5527776682455eb8f90a5b688df22f",
            "fd7f7dc02d644650bb00100f0f63aa07",
            "54a6e9c1779f480f9bb1594838291607",
            "72c7aa9180434d078696416893683137",
            "b199a1d8681e4df9a7b83841890e9a7d",
            "ecc2f4a4edcd43139aee28b4a962e20f",
            "26e030d7be0343678d18564ca4e61abe",
            "307c1b938ff74d59bd7396d5a75591b7",
            "9bfb16c4ca414e01a608b365ed34ef18",
            "473afd9fee9942c99b777456780fee83",
            "222e0077e7b54ca59e82f53eab701a3f"
          ]
        },
        "id": "XhfwJ2apV3X8",
        "outputId": "07b44e89-6367-4f04-bcb5-38b54363344f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32dde54f17f1456eb4d2fc20668c7c24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2be75242495e42618a770fec255199c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b20644889614d9597883b6c7ed7c4b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "528af380945445aba679e635bfd113c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d947e3eac0164714a74011f35d0af3c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be5527776682455eb8f90a5b688df22f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              type                                       question  \\\n",
            "3529  MULTI-SELECT         What algorithms are you familiar with?   \n",
            "3530  MULTI-SELECT         What algorithms are you familiar with?   \n",
            "3531  MULTI-SELECT         What algorithms are you familiar with?   \n",
            "3532  MULTI-SELECT         What algorithms are you familiar with?   \n",
            "3533  MULTI-SELECT         What algorithms are you familiar with?   \n",
            "...            ...                                            ...   \n",
            "4446  MULTI-SELECT  Which operating systems do you use regularly?   \n",
            "4447  MULTI-SELECT  Which operating systems do you use regularly?   \n",
            "4448  MULTI-SELECT  Which operating systems do you use regularly?   \n",
            "4449  MULTI-SELECT  Which operating systems do you use regularly?   \n",
            "4450  MULTI-SELECT  Which operating systems do you use regularly?   \n",
            "\n",
            "                                                options  \\\n",
            "3529  [Sorting algorithms,  Searching algorithms,  G...   \n",
            "3530  [Sorting algorithms,  Searching algorithms,  G...   \n",
            "3531  [Sorting algorithms,  Searching algorithms,  G...   \n",
            "3532  [Sorting algorithms,  Searching algorithms,  G...   \n",
            "3533  [Sorting algorithms,  Searching algorithms,  G...   \n",
            "...                                                 ...   \n",
            "4446          [Windows,  macOS,  Linux,  Android,  iOS]   \n",
            "4447          [Windows,  macOS,  Linux,  Android,  iOS]   \n",
            "4448          [Windows,  macOS,  Linux,  Android,  iOS]   \n",
            "4449          [Windows,  macOS,  Linux,  Android,  iOS]   \n",
            "4450          [Windows,  macOS,  Linux,  Android,  iOS]   \n",
            "\n",
            "                                                 answer  \\\n",
            "3529  I'm familiar with a wide range of algorithms, ...   \n",
            "3530          Sorting, searching, and graph algorithms.   \n",
            "3531  I know a bunch of those, like the sorting and ...   \n",
            "3532  My algorithm knowledge encompasses several cat...   \n",
            "3533  I know about sorting, searching, and divide an...   \n",
            "...                                                 ...   \n",
            "4446  I primarily use Windows for work, as it's the ...   \n",
            "4447                                  Windows, Android.   \n",
            "4448  I'm all about Windows and Android, that's my jam.   \n",
            "4449  My daily driver is Windows 10 for its compatib...   \n",
            "4450  I use Windows at work, and Android on my phone...   \n",
            "\n",
            "                                    answer_label_GEMINI answer_category  \n",
            "3529  [Sorting algorithms, Searching algorithms, Gra...         verbose  \n",
            "3530  [Sorting algorithms, Searching algorithms, Gra...         concise  \n",
            "3531  [Sorting algorithms, Searching algorithms, Gra...      colloquial  \n",
            "3532  [Sorting algorithms, Searching algorithms, Dyn...     explanatory  \n",
            "3533  [Sorting algorithms, Searching algorithms, Div...         annoyed  \n",
            "...                                                 ...             ...  \n",
            "4446                              Windows, macOS, Linux         verbose  \n",
            "4447                                   Windows, Android         concise  \n",
            "4448                                   Windows, Android      colloquial  \n",
            "4449                                       Windows, iOS     explanatory  \n",
            "4450                                   Windows, Android         annoyed  \n",
            "\n",
            "[922 rows x 6 columns]\n",
            "[' Searching algorithms', ' Graph algorithms', 'Sorting algorithms']\n",
            "[' Graph algorithms', ' Searching algorithms', 'Sorting algorithms']\n",
            "[' Dynamic programming', ' Searching algorithms', 'Sorting algorithms', ' Graph algorithms']\n",
            "[' Greedy algorithms', ' Searching algorithms', 'Sorting algorithms', ' Dynamic programming']\n",
            "[' Divide and conquer', 'Sorting algorithms', ' Searching algorithms']\n",
            "[' Technology changes', 'Competition', ' Economic conditions']\n",
            "['Competition', ' Finding and retaining talent']\n",
            "[' Economic conditions', ' Finding and retaining talent', 'Competition']\n",
            "[' Technology changes', ' Regulations']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Regulations', ' Economic conditions']\n",
            "[' Need', ' Industry', 'Company size', ' Budget', ' Authority']\n",
            "[' Need', ' Budget']\n",
            "[' Need', ' Budget', ' Industry']\n",
            "[' Need', ' Budget', ' Authority']\n",
            "[' Budget', ' Need']\n",
            "[' Conversion rate', ' Customer lifetime value (CLTV)', 'Revenue']\n",
            "['Revenue', ' Conversion rate']\n",
            "['Revenue', ' Conversion rate']\n",
            "[' Customer acquisition cost (CAC)', ' Average deal size', 'Revenue']\n",
            "[' Average deal size', 'Revenue']\n",
            "[' High customer acquisition costs', 'Lack of leads']\n",
            "[' High customer acquisition costs', ' Competition']\n",
            "['Lack of leads', ' Low conversion rates', ' High customer acquisition costs']\n",
            "[' Low conversion rates', ' Competition']\n",
            "[' Lack of resources', ' Competition', 'Lack of leads', ' Low conversion rates', ' High customer acquisition costs']\n",
            "['Objections from prospects', ' Price sensitivity', ' Long sales cycles', ' Competition']\n",
            "[' Long sales cycles', ' Price sensitivity']\n",
            "[' Long sales cycles', ' Price sensitivity', ' Competition']\n",
            "['Objections from prospects']\n",
            "[' Forecasting sales accurately', ' Identifying and addressing bottlenecks', ' Prioritizing deals effectively', ' Maintaining pipeline visibility', 'Tracking deal progress']\n",
            "[' Prioritizing deals effectively', ' Maintaining pipeline visibility', 'Tracking deal progress', ' Forecasting sales accurately']\n",
            "['Tracking deal progress']\n",
            "[' Forecasting sales accurately', ' Maintaining pipeline visibility', 'Tracking deal progress', ' Prioritizing deals effectively']\n",
            "['Tracking deal progress', ' Maintaining pipeline visibility', ' Forecasting sales accurately', ' Prioritizing deals effectively']\n",
            "[' Ensuring consistent performance', 'Maintaining team motivation', ' Providing adequate training and development', ' Tracking team progress']\n",
            "[' Managing team conflicts', ' Ensuring consistent performance', ' Tracking team progress', ' Providing adequate training and development', 'Maintaining team motivation']\n",
            "['Maintaining team motivation', ' Managing team conflicts', ' Ensuring consistent performance', ' Tracking team progress']\n",
            "[' Providing adequate training and development', ' Ensuring consistent performance', ' Managing team conflicts', ' Tracking team progress']\n",
            "[' Ensuring consistent performance', ' Tracking team progress', 'Maintaining team motivation', ' Managing team conflicts', ' Providing adequate training and development']\n",
            "[' Managing increasing sales volume', 'Hiring and training new sales reps', ' Managing sales team growth']\n",
            "[' Managing sales team growth', ' Maintaining consistent sales performance', ' Managing increasing sales volume', 'Hiring and training new sales reps']\n",
            "[' Managing sales team growth', 'Hiring and training new sales reps', ' Managing increasing sales volume', ' Maintaining consistent sales performance']\n",
            "[' Maintaining consistent sales performance', ' Managing sales team growth', ' Managing increasing sales volume', ' Scaling your sales technology', 'Hiring and training new sales reps']\n",
            "[' Managing increasing sales volume', ' Managing sales team growth', ' Maintaining consistent sales performance', 'Hiring and training new sales reps', ' Scaling your sales technology']\n",
            "[' Skills gap', 'Automation']\n",
            "[' Gig economy', ' Remote work', 'Automation', ' Artificial intelligence']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dbf0880f2967>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Apply the classification to each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf_MULTI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer_label_BART_large_mnli\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_MULTI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclassify_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Display the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10373\u001b[0m         )\n\u001b[0;32m> 10374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10376\u001b[0m     def map(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dbf0880f2967>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Apply the classification to each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf_MULTI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer_label_BART_large_mnli\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_MULTI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclassify_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Display the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dbf0880f2967>\u001b[0m in \u001b[0;36mclassify_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Function to classify each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"options\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfiltered_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to understand extra arguments {args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"This example is {}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m             return next(\n\u001b[0m\u001b[1;32m   1294\u001b[0m                 iter(\n\u001b[1;32m   1295\u001b[0m                     self.get_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         model_outputs = {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1761\u001b[0m             )\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1764\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1511\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1113\u001b[0m                     )\n\u001b[1;32m   1114\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m                     layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m   1116\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \"\"\"\n\u001b[1;32m    567\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         hidden_states, attn_weights, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;31m# self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_full"
      ],
      "metadata": {
        "id": "5RD5IZEdCkjI",
        "outputId": "94de5909-fd57-4c72-ded2-c64d071f7d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6087
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              type                                           question  \\\n",
              "0             type                                           question   \n",
              "1             TEXT  Describe any areas where cost savings could be...   \n",
              "2             TEXT  Describe any areas where cost savings could be...   \n",
              "3             TEXT  Describe any areas where cost savings could be...   \n",
              "4             TEXT  Describe any areas where cost savings could be...   \n",
              "...            ...                                                ...   \n",
              "4446  MULTI-SELECT      Which operating systems do you use regularly?   \n",
              "4447  MULTI-SELECT      Which operating systems do you use regularly?   \n",
              "4448  MULTI-SELECT      Which operating systems do you use regularly?   \n",
              "4449  MULTI-SELECT      Which operating systems do you use regularly?   \n",
              "4450  MULTI-SELECT      Which operating systems do you use regularly?   \n",
              "\n",
              "                                        options  \\\n",
              "0                                     [options]   \n",
              "1                                        [Text]   \n",
              "2                                        [Text]   \n",
              "3                                        [Text]   \n",
              "4                                        [Text]   \n",
              "...                                         ...   \n",
              "4446  [Windows,  macOS,  Linux,  Android,  iOS]   \n",
              "4447  [Windows,  macOS,  Linux,  Android,  iOS]   \n",
              "4448  [Windows,  macOS,  Linux,  Android,  iOS]   \n",
              "4449  [Windows,  macOS,  Linux,  Android,  iOS]   \n",
              "4450  [Windows,  macOS,  Linux,  Android,  iOS]   \n",
              "\n",
              "                                                 answer  \\\n",
              "0                                                answer   \n",
              "1      Significant cost savings could be realized by...   \n",
              "2      Reduce supplier costs, improve energy efficie...   \n",
              "3       Dude, we could totally save a ton of cash by...   \n",
              "4      Cost savings can be achieved through a multi-...   \n",
              "...                                                 ...   \n",
              "4446  I primarily use Windows for work, as it's the ...   \n",
              "4447                                  Windows, Android.   \n",
              "4448  I'm all about Windows and Android, that's my jam.   \n",
              "4449  My daily driver is Windows 10 for its compatib...   \n",
              "4450  I use Windows at work, and Android on my phone...   \n",
              "\n",
              "                                    answer_label_GEMINI  answer_category  \\\n",
              "0                                   answer_label_GEMINI  answer_category   \n",
              "1     Supplier negotiation, energy efficiency, proce...          verbose   \n",
              "2                             Cost reduction strategies          concise   \n",
              "3     Supplier negotiation, energy saving, waste red...       colloquial   \n",
              "4     Contract review, energy efficiency, workflow o...      explanatory   \n",
              "...                                                 ...              ...   \n",
              "4446                              Windows, macOS, Linux          verbose   \n",
              "4447                                   Windows, Android          concise   \n",
              "4448                                   Windows, Android       colloquial   \n",
              "4449                                       Windows, iOS      explanatory   \n",
              "4450                                   Windows, Android          annoyed   \n",
              "\n",
              "        predicted_options  \n",
              "0                     NaN  \n",
              "1                     NaN  \n",
              "2                     NaN  \n",
              "3                     NaN  \n",
              "4                     NaN  \n",
              "...                   ...  \n",
              "4446    [Windows,  macOS]  \n",
              "4447            [Windows]  \n",
              "4448  [Windows,  Android]  \n",
              "4449      [Windows,  iOS]  \n",
              "4450  [Windows,  Android]  \n",
              "\n",
              "[4476 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7570878b-ed39-4b90-b150-d678b9e839c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>question</th>\n",
              "      <th>options</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_label_GEMINI</th>\n",
              "      <th>answer_category</th>\n",
              "      <th>predicted_options</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>type</td>\n",
              "      <td>question</td>\n",
              "      <td>[options]</td>\n",
              "      <td>answer</td>\n",
              "      <td>answer_label_GEMINI</td>\n",
              "      <td>answer_category</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEXT</td>\n",
              "      <td>Describe any areas where cost savings could be...</td>\n",
              "      <td>[Text]</td>\n",
              "      <td>Significant cost savings could be realized by...</td>\n",
              "      <td>Supplier negotiation, energy efficiency, proce...</td>\n",
              "      <td>verbose</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEXT</td>\n",
              "      <td>Describe any areas where cost savings could be...</td>\n",
              "      <td>[Text]</td>\n",
              "      <td>Reduce supplier costs, improve energy efficie...</td>\n",
              "      <td>Cost reduction strategies</td>\n",
              "      <td>concise</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEXT</td>\n",
              "      <td>Describe any areas where cost savings could be...</td>\n",
              "      <td>[Text]</td>\n",
              "      <td>Dude, we could totally save a ton of cash by...</td>\n",
              "      <td>Supplier negotiation, energy saving, waste red...</td>\n",
              "      <td>colloquial</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEXT</td>\n",
              "      <td>Describe any areas where cost savings could be...</td>\n",
              "      <td>[Text]</td>\n",
              "      <td>Cost savings can be achieved through a multi-...</td>\n",
              "      <td>Contract review, energy efficiency, workflow o...</td>\n",
              "      <td>explanatory</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4446</th>\n",
              "      <td>MULTI-SELECT</td>\n",
              "      <td>Which operating systems do you use regularly?</td>\n",
              "      <td>[Windows,  macOS,  Linux,  Android,  iOS]</td>\n",
              "      <td>I primarily use Windows for work, as it's the ...</td>\n",
              "      <td>Windows, macOS, Linux</td>\n",
              "      <td>verbose</td>\n",
              "      <td>[Windows,  macOS]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4447</th>\n",
              "      <td>MULTI-SELECT</td>\n",
              "      <td>Which operating systems do you use regularly?</td>\n",
              "      <td>[Windows,  macOS,  Linux,  Android,  iOS]</td>\n",
              "      <td>Windows, Android.</td>\n",
              "      <td>Windows, Android</td>\n",
              "      <td>concise</td>\n",
              "      <td>[Windows]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4448</th>\n",
              "      <td>MULTI-SELECT</td>\n",
              "      <td>Which operating systems do you use regularly?</td>\n",
              "      <td>[Windows,  macOS,  Linux,  Android,  iOS]</td>\n",
              "      <td>I'm all about Windows and Android, that's my jam.</td>\n",
              "      <td>Windows, Android</td>\n",
              "      <td>colloquial</td>\n",
              "      <td>[Windows,  Android]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4449</th>\n",
              "      <td>MULTI-SELECT</td>\n",
              "      <td>Which operating systems do you use regularly?</td>\n",
              "      <td>[Windows,  macOS,  Linux,  Android,  iOS]</td>\n",
              "      <td>My daily driver is Windows 10 for its compatib...</td>\n",
              "      <td>Windows, iOS</td>\n",
              "      <td>explanatory</td>\n",
              "      <td>[Windows,  iOS]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4450</th>\n",
              "      <td>MULTI-SELECT</td>\n",
              "      <td>Which operating systems do you use regularly?</td>\n",
              "      <td>[Windows,  macOS,  Linux,  Android,  iOS]</td>\n",
              "      <td>I use Windows at work, and Android on my phone...</td>\n",
              "      <td>Windows, Android</td>\n",
              "      <td>annoyed</td>\n",
              "      <td>[Windows,  Android]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4476 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7570878b-ed39-4b90-b150-d678b9e839c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7570878b-ed39-4b90-b150-d678b9e839c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7570878b-ed39-4b90-b150-d678b9e839c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}